# Simple Processes

Here we have two very simple scripts to be run as processes.

- `process1.py` simulates an accumulator process (not service!) that generates one data file and stores it on MinIO. This process should run on the Cloud.
- `process2.py` simulates an ML training process that gets this data file generated by process1, generates an ML model, and stores it back in the MinIO. This process should run as an HPC job on the HPC system.

This how it's expected to work:

1. This script should run in a container on the OpenShift cluster

```shell
$> python process1.py 
This is the generated file_id: abcdef    # This means that the file has been generated and successfully stored in MinIO 
{"file_id": "abcdef"} 
```

2. This script should run on the HPC system (as a job)

```shell
$> python process2.py abcdef
The new file_id for the modified file is: xyz    # This means that a new file has been generated and stored back into Minio. 
{"file_id": "xyz"} 
```

# Image Building using Image Stream

- oc apply -f imagestream.yaml
- oc apply -f buildconfig.yaml
- oc start-build process1 --from-dir=.  # This will update the image with the current code
- Run the pod using ./run_pod.sh

You can use the script ./deploy.sh to run these steps above.